
# Custom configuration for StormCrawler
# This is used to override the default values from crawler-default.xml and provide additional ones 
# for your custom components.
# Use this file with the parameter -conf when launching your extension of ConfigurableTopology.
# This file does not contain all the key values but only the most frequently used ones. See crawler-default.xml for an extensive list.

config: 
  topology.workers: 1
  # topology timeout also defines the total time allowed to fetch a single page
  topology.message.timeout.secs: 300
  topology.max.spout.pending: 100
  topology.debug: false

  topology.name: "NewsCrawl"

  # mandatory when using Flux
  topology.kryo.register:
    - com.digitalpebble.stormcrawler.Metadata

  topology.backpressure.enable: false
  
  # set to 0 to deactivate debugging
  topology.eventlogger.executors: 0
  
  #Metrics consumers:
  topology.metrics.consumer.register:
     - class: "org.apache.storm.metric.LoggingMetricsConsumer"
       parallelism.hint: 1
     - class: "com.digitalpebble.stormcrawler.elasticsearch.metrics.MetricsConsumer"
       parallelism.hint: 1

  partition.url.mode: "byDomain"

  http.content.limit: 1048576

  # store partial fetches as trimmed content (some content has been fetched,
  # but reading more data from socket failed, eg. because of a network timeout)
  http.content.partial.as.trimmed: true

  # store HTTP headers (required for WARC files)
  http.store.headers: true

  urlfilters.config.file: "urlfilters.json"
  parsefilters.config.file: "parsefilters.json"
  
  # time bucket to use for the metrics sent by the Fetcher
  fetcher.metrics.time.bucket.secs: 60

  fetcher.threads.number: 80

  worker.heap.memory.mb: 4096

  # increased network timeout (ms) for news sites from Asia and eastern Europe
  http.timeout: 30000

  # use okhttp
  http.protocol.implementation: com.digitalpebble.stormcrawler.protocol.okhttp.HttpProtocol
  https.protocol.implementation: com.digitalpebble.stormcrawler.protocol.okhttp.HttpProtocol

  # do not fail on unknown SSL certificates
  http.trust.everything: true

  # use okhttp
  http.protocol.implementation: com.digitalpebble.stormcrawler.protocol.okhttp.HttpProtocol
  https.protocol.implementation: com.digitalpebble.stormcrawler.protocol.okhttp.HttpProtocol

  # delay between successive requests to the same host
  # (be defensive, a delay of 5 sec. means about 1000 fetches per hour
  #  which should be enough even for large news sites)
  fetcher.server.delay: 3.0

  # generous max. crawl delay
  # (fetch content even if the robots.txt specifies a large host-specific crawl delay:
  #  waiting 90 sec. between successive fetches would still allow to fetch
  #  about 1000 pages per day, enough for small news sites)
  fetcher.max.crawl.delay: 120

  # limit the number of queued URLs
  # - avoid duplicate fetches (queues are not sets)
  fetcher.max.queue.size: 10
  fetcher.max.urls.in.queues: 3000

  # fetch Scheduler implementation
  scheduler.class: "com.digitalpebble.stormcrawler.persistence.AdaptiveScheduler"
  # AdaptiveScheduler properties
  scheduler.adaptive.setLastModified: true
  # frequently changing feeds or news sitemaps are refetched after 90 min.
  scheduler.adaptive.fetchInterval.min: 90
  # if there are no changes the interval may grow to 90 days
  scheduler.adaptive.fetchInterval.max: 129600
  scheduler.adaptive.fetchInterval.rate.incr: .5
  scheduler.adaptive.fetchInterval.rate.decr: .2

  # AdaptiveScheduler only handles successful re-fetches (HTTP 200 or 304 not modified),
  # all others are delegated to DefaultScheduler

  # revisit a page (value in minutes)
  # - never revisit a news article
  #   (seeds / RSS feeds are revisited)
  # - 10 years (-1 would not allow to distinguish the fetch time, required for clean-ups)
  fetchInterval.default: 5256000
  
  # revisit a page with a fetch error after 2 hours (value in minutes)
  fetchInterval.fetch.error: 120
  
  # revisit a page with an error after 3 month (value in minutes)
  fetchInterval.error: 133920

  # custom interval for feeds and sitemaps

  # try to re-fetch failed feeds or sitemaps after 2 month
  # (status is neither FETCHED, REDIRECTION, or DISCOVERED)
  fetchInterval.FETCH_ERROR.isFeed=true: 86400
  fetchInterval.FETCH_ERROR.isSitemapNews=true: 86400
  fetchInterval.FETCH_ERROR.isSitemapIndex=true: 86400
  fetchInterval.ERROR.isFeed=true: 86400
  fetchInterval.ERROR.isSitemapNews=true: 86400
  fetchInterval.ERROR.isSitemapIndex=true: 86400

  # re-fetch redirected feeds or sitemaps after 2 weeks
  # (the redirect target may have changed)
  fetchInterval.REDIRECTION.isFeed=true: 20160
  fetchInterval.REDIRECTION.isSitemapNews=true: 20160
  fetchInterval.REDIRECTION.isSitemapIndex=true: 20160

  # re-fetch feeds 2 times per day by default (adaptive scheduler will set a lower frequency if needed)
  fetchInterval.FETCHED.isFeed=true: 720

  # news sitemaps may contain up to 1000 URLs, re-fetch daily
  fetchInterval.FETCHED.isSitemapNews=true: 1440

  # a sitemap index may contain a new link to a news sitemap
  # but usually more links to non-news sitemaps, re-fetch weekly
  fetchInterval.FETCHED.isSitemapIndex=true: 10080

  # retry a sitemap after 6 month because it may change and
  # may turn into a news sitemap by adding namespace and annotations
  fetchInterval.FETCHED.isSitemap=true: 259200
  fetchInterval.REDIRECTION.isSitemap=true: 259200
  fetchInterval.FETCH_ERROR.isSitemap=true: 259200
  fetchInterval.ERROR.isSitemap=true: 259200

  # must specify a zero interval for newly discovered or injected
  # feeds and sitemaps, otherwise the default interval is added.
  fetchInterval.DISCOVERED.isFeed=true: 0
  fetchInterval.DISCOVERED.isSitemapNews=true: 0
  fetchInterval.DISCOVERED.isSitemapIndex=true: 0
  # wait 5 min. for auto-detected sitemaps
  fetchInterval.DISCOVERED.isSitemap=true: 300

  # auto-detect RSS feeds
  feed.sniffContent: true

  # auto-detection of news sitemaps
  sitemap.sniffContent: true
  sitemap.discovery: true
  sitemap.offset.guess: 1024

  # delay each detected subsitemap for 15 min. before being fetched
  # after the previous one so that a large sitemap index "floods"
  # the status index slowly. A sitemap index is allowed to link
  # to 50,000 sitemaps, and each sitemap can contain up to 50,000
  # URLs.
  sitemap.schedule.delay: 15

  # do not add feeds older than 30 days
  # (longest adaptive fetch interval is 28 days)
  feed.filter.hours.since.published: 720
  # same for news sitemaps
  sitemap.filter.hours.since.modified: 720

  # no cache
  #  - ES is running locally and should be fast
  #  - not following links only accessing news feeds and sitemaps 
  status.updater.use.cache: false
  status.updater.cache.spec: "maximumSize=250000,expireAfterAccess=4h"

  # lists the metadata to persist to storage
  # these are not transfered to the outlinks
  metadata.persist:
   - _redirTo
   - fetch.statusCode
   - error.cause
   - error.source
   - isSitemap
   - isSitemapNews
   - isSitemapIndex
   - isFeed
   - numLinks
   - last-modified
   - signature
   - signatureChangeDate
   - fetchInterval
  # whether to track the URL path
  # (allows to identify which feeds and sitemaps contribute links)
  metadata.track.path: true

  # HTTP 'User-Agent' request header
  # NOTE: must set these configuration properties to identify your bot!
  http.agent.name: ""
  http.agent.version: ""
  http.agent.description: ""
  http.agent.url: ""
  http.agent.email: ""

  # change to the location of your choice
  # the directory must already exist
  warc.dir: "/data/warc"
  # rotate WARC files shortly before 1 GB is reached
  warc.rotation.policy.max-mb: 1023
  # but latest after one day
  warc.rotation.policy.max-minutes: 1440



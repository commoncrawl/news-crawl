
# Custom configuration for StormCrawler
# This is used to override the default values from crawler-default.xml and provide additional ones
# for your custom components.
# Use this file with the parameter -conf when launching your extension of ConfigurableTopology.
# This file does not contain all the key values but only the most frequently used ones. See crawler-default.xml for an extensive list.

config:
  topology.workers: 1
  # topology timeout also defines the total time allowed to fetch a single page
  topology.message.timeout.secs: 300
  topology.max.spout.pending: 100
  topology.debug: false

  # override the JVM parameters for the workers
  topology.worker.childopts: "-Xmx4g -Djava.net.preferIPv4Stack=true"

  topology.name: "NewsCrawl"

  # mandatory when using Flux
  topology.kryo.register:
    - com.digitalpebble.stormcrawler.Metadata

  topology.backpressure.enable: false

  # set to 0 to deactivate debugging
  topology.eventlogger.executors: 0

  #Metrics consumers:
  topology.metrics.consumer.register:
     - class: "org.apache.storm.metric.LoggingMetricsConsumer"
       parallelism.hint: 1
     - class: "com.digitalpebble.stormcrawler.elasticsearch.metrics.MetricsConsumer"
       parallelism.hint: 1

  # status index and fetcher queues are partitioned by domain
  partition.url.mode: "byDomain"

  # The maximum number of bytes for returned HTTP response bodies.
  # The fetched page will be trimmed to 65KB in this case
  # Set -1 to disable the limit.
  # Content payload of WARC records is limited to 1 MiB:
  http.content.limit: 1048576

  # store partial fetches as trimmed content (some content has been fetched,
  # but reading more data from socket failed, eg. because of a network timeout)
  http.content.partial.as.trimmed: true

  # store HTTP headers (required for WARC files)
  http.store.headers: true

  # FetcherBolt queue dump => comment out to activate
  # if a file exists on the worker machine with the corresponding port number
  # the FetcherBolt will log the content of its internal queues to the logs
  # fetcherbolt.queue.debug.filepath: "/tmp/fetcher-dump-{port}"

  parsefilters.config.file: "parsefilters.json"
  urlfilters.config.file: "urlfilters.json"

  # time bucket to use for the metrics sent by the Fetcher
  fetcher.metrics.time.bucket.secs: 60

  fetcher.threads.number: 80

  # increased network timeout (ms) for news sites from Asia and eastern Europe
  http.timeout: 30000

  # use okhttp
  http.protocol.implementation: com.digitalpebble.stormcrawler.protocol.okhttp.HttpProtocol
  https.protocol.implementation: com.digitalpebble.stormcrawler.protocol.okhttp.HttpProtocol

  # do not fail on unknown SSL certificates
  http.trust.everything: true

  # key values obtained by the protocol can be prefixed
  # to avoid accidental overwrites. Note that persisted
  # or transferred protocol metadata must also be prefixed.
  protocol.md.prefix: "protocol."

  # delay between successive requests to the same host/domain
  # (be defensive, a delay of 5 sec. means about 1000 fetches per hour
  #  which should be enough even for large news sites)
  fetcher.server.delay: 6.0

  # generous max. crawl delay
  # (fetch content even if the robots.txt specifies a large host-specific crawl delay:
  #  waiting 90 sec. between successive fetches would still allow to fetch
  #  about 1000 pages per day, enough for small news sites)
  fetcher.max.crawl.delay: 120

  # use the larger default delay (fetcher.server.delay)
  # in case a shorter crawl-delay is defined in the robots.txt
  fetcher.server.delay.force: true

  # limit the number of queued URLs
  # - avoid duplicate fetches (queues are not sets)
  fetcher.max.queue.size: 10
  fetcher.max.urls.in.queues: 6000

  # fetch Scheduler implementation
  scheduler.class: "com.digitalpebble.stormcrawler.persistence.AdaptiveScheduler"
  # AdaptiveScheduler properties
  scheduler.adaptive.setLastModified: true
  # frequently changing feeds or news sitemaps are refetched after 90 min.
  scheduler.adaptive.fetchInterval.min: 90
  # if there are no changes the interval may grow to 90 days
  scheduler.adaptive.fetchInterval.max: 129600
  scheduler.adaptive.fetchInterval.rate.incr: .5
  scheduler.adaptive.fetchInterval.rate.decr: .2

  # AdaptiveScheduler only handles successful re-fetches (HTTP 200 or 304 not modified),
  # all others are delegated to DefaultScheduler

  # revisit a page (value in minutes)
  # - never revisit a news article
  #   (seeds / RSS feeds are revisited)
  # - 10 years (-1 would not allow to distinguish the fetch time, required for clean-ups)
  fetchInterval.default: 5256000

  # revisit a page with a transient fetch error after 48 hours (value in minutes)
  # - choose a larger interval (2 days instead of 2 hours) to give the server
  #   enough time to recover
  # - this is important for seeds and sitemaps which may flip into error state
  #   if the refetch interval is too short in case the server is unavailable for
  #     (max.fetch.errors - 1) * fetchInterval.fetch.error
  fetchInterval.fetch.error: 2880

  # revisit a page with an error after 3 month (value in minutes)
  fetchInterval.error: 133920

  # custom interval for feeds and sitemaps

  # try to re-fetch failed feeds or sitemaps after 3 days
  fetchInterval.FETCH_ERROR.isFeed=true: 4320
  fetchInterval.FETCH_ERROR.isSitemapNews=true: 4320
  fetchInterval.FETCH_ERROR.isSitemapIndex=true: 4320

  # try to re-fetch feeds or sitemaps with status 404 or equiv. after 2 months
  # (status is neither FETCHED, REDIRECTION, or DISCOVERED)
  fetchInterval.ERROR.isFeed=true: 86400
  fetchInterval.ERROR.isSitemapNews=true: 86400
  fetchInterval.ERROR.isSitemapIndex=true: 86400

  # re-fetch redirected feeds or sitemaps after 2 weeks
  # (the redirect target may have changed)
  fetchInterval.REDIRECTION.isFeed=true: 20160
  fetchInterval.REDIRECTION.isSitemapNews=true: 20160
  fetchInterval.REDIRECTION.isSitemapIndex=true: 20160

  # re-fetch feeds 2 times per day by default (adaptive scheduler will set a lower frequency if needed)
  fetchInterval.FETCHED.isFeed=true: 720

  # news sitemaps may contain up to 1000 URLs, re-fetch daily
  fetchInterval.FETCHED.isSitemapNews=true: 1440
  fetchInterval.FETCHED.isSitemapVerified=true: 1440

  # a sitemap index may contain a new link to a news sitemap
  # but usually more links to non-news sitemaps, re-fetch weekly
  fetchInterval.FETCHED.isSitemapIndex=true: 10080

  # retry a sitemap after 6 month because it may change and
  # may turn into a news sitemap by adding namespace and annotations
  fetchInterval.FETCHED.isSitemap=true: 259200
  fetchInterval.REDIRECTION.isSitemap=true: 259200
  fetchInterval.FETCH_ERROR.isSitemap=true: 259200
  fetchInterval.ERROR.isSitemap=true: 259200

  # must specify a zero interval for newly discovered or injected
  # feeds and sitemaps, otherwise the default interval is added.
  fetchInterval.DISCOVERED.isFeed=true: 0
  fetchInterval.DISCOVERED.isSitemapNews=true: 0
  fetchInterval.DISCOVERED.isSitemapIndex=true: 0
  # wait 5 min. for auto-detected sitemaps
  fetchInterval.DISCOVERED.isSitemap=true: 300

  # auto-detect RSS feeds
  feed.sniffContent: true

  # auto-detection of news sitemaps
  sitemap.sniffContent: true
  sitemap.discovery: true
  sitemap.offset.guess: 1024

  # delay each detected subsitemap for 15 min. before being fetched
  # after the previous one so that a large sitemap index "floods"
  # the status index slowly. A sitemap index is allowed to link
  # to 50,000 sitemaps, and each sitemap can contain up to 50,000
  # URLs.
  sitemap.schedule.delay: 15

  # do not add feeds older than 30 days
  # (longest adaptive fetch interval is 28 days)
  feed.filter.hours.since.published: 720
  # same for news sitemaps
  sitemap.filter.hours.since.modified: 720

  # no cache
  #  - ES is running locally and should be fast
  #  - not following links only accessing news feeds and sitemaps
  status.updater.use.cache: false
  status.updater.cache.spec: "maximumSize=250000,expireAfterAccess=4h"

  # metadata to transfer to the outlinks
  # used by Fetcher for redirections, sitemapparser,
  # passing cookies to child pages, etc.
  # These are also persisted for the parent document (see below)
  #metadata.transfer:
  # - protocol.set-cookie

  # lists the metadata to persist to storage
  # these are not transferred to the outlinks
  metadata.persist:
   - _redirTo
   - fetch.statusCode
   - error.cause
   - error.source
   - isSitemap
   - isSitemapNews
   - isSitemapIndex
   - isSitemapVerified
   - isFeed
   - numLinks
   - last-modified
   - signature
   - signatureChangeDate
   - fetchInterval
   - protocol.etag
  # whether to track the URL path
  # (allows to identify which feeds and sitemaps contribute links)
  metadata.track.path: true
  metadata.track.depth: false

  # HTTP 'User-Agent' request header
  # NOTE: must set these configuration properties to identify your bot!
  http.agent.name: ""
  http.agent.version: ""
  http.agent.description: ""
  http.agent.url: ""
  http.agent.email: ""

  # change to the location of your choice
  # the directory must already exist
  warc.dir: "/data/warc"
  # rotate WARC files shortly before 1 GB is reached
  warc.rotation.policy.max-mb: 1023
  # but latest after one day
  warc.rotation.policy.max-minutes: 1440


